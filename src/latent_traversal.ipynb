{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize\n",
    "from utils import *\n",
    "from models.closedform.utils import load_generator\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18\n",
    "import torch.nn as nn\n",
    "import matplotlib.pylab as plt\n",
    "import torchvision\n",
    "import cv2\n",
    "from IPython import display\n",
    "from evaluation import Evaluator\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SRE Rescoring Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# all_attr_list = ['pose', 'young', 'male', 'smiling', 'eyeglasses', 'Bald',\n",
    "#                               'Sideburns', 'Wearing_Lipstick', 'Pale_Skin',\n",
    "#                               'No_Beard', 'Wearing_Hat', 'Goatee',\n",
    "#                               'Mustache', 'Double_Chin', 'Gray_Hair',\n",
    "#                               'Wearing_Necktie', 'Blurry', 'Bangs']\n",
    "# attr_index = list(range(len(all_attr_list)))\n",
    "# attr_list_dict = OrderedDict(zip(all_attr_list, attr_index))\n",
    "\n",
    "# attributes = ['pose', 'male', 'young', 'smiling', 'eyeglasses']\n",
    "# root_dir = '/home/adarsh/PycharmProjects/disentagled_latent_dirs'\n",
    "# result_path = os.path.join(root_dir,  'results/celeba_hq/closed_form_ours/quantitative_analysis')\n",
    "# rescoring_matrix = torch.load(os.path.join(result_path, 'rescoring matrix.pkl'))\n",
    "# attr_manipulation_acc = torch.load(os.path.join(result_path, 'attribute manipulation accuracy.pkl'))\n",
    "# direction_idx = [2, 1, 11, 5, 14]\n",
    "# attr_vs_direction = OrderedDict(zip(attributes, direction_idx))\n",
    "# rescoring_labels = ['Pose', 'Gender', 'Age', 'Smile', 'Glasses']\n",
    "# Evaluator.get_partial_metrics(result_path, attributes, direction_idx, attr_list_dict, attr_vs_direction, rescoring_matrix,\n",
    "#                               rescoring_labels, attr_manipulation_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CF Rescoring Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# all_attr_list = ['pose', 'young', 'male', 'smiling', 'eyeglasses', 'Bald',\n",
    "#                               'Sideburns', 'Wearing_Lipstick', 'Pale_Skin',\n",
    "#                               'No_Beard', 'Wearing_Hat', 'Goatee',\n",
    "#                               'Mustache', 'Double_Chin', 'Gray_Hair',\n",
    "#                               'Wearing_Necktie', 'Blurry', 'Bangs']\n",
    "# attr_index = list(range(len(all_attr_list)))\n",
    "# attr_list_dict = OrderedDict(zip(all_attr_list, attr_index))\n",
    "\n",
    "# attributes = ['pose', 'male', 'young', 'smiling', 'eyeglasses']\n",
    "# root_dir = '/home/adarsh/PycharmProjects/disentagled_latent_dirs'\n",
    "# result_path = os.path.join(root_dir,  'results/celeba_hq/closed_form/quantitative_analysis')\n",
    "# rescoring_matrix = torch.load(os.path.join(result_path, 'rescoring matrix.pkl'))\n",
    "# attr_manipulation_acc = torch.load(os.path.join(result_path, 'attribute manipulation accuracy.pkl'))\n",
    "# direction_idx = [2, 1, 11, 4, 1]\n",
    "# attr_vs_direction = OrderedDict(zip(attributes, direction_idx))\n",
    "# rescoring_labels = ['Pose', 'Gender', 'Age', 'Smile', 'Glasses']\n",
    "# Evaluator.get_partial_metrics(result_path, attributes, direction_idx, attr_list_dict, attr_vs_direction, rescoring_matrix,\n",
    "#                               rescoring_labels, attr_manipulation_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Traversals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1234\n",
    "set_seed(random_seed)\n",
    "load_codes = True\n",
    "algo = 'ortho'\n",
    "root_dir= '/home/adarsh/PycharmProjects/disentagled_latent_dirs'\n",
    "result_path = os.path.join(root_dir,  'results/celeba_hq/closed_form_ours/qualitative_analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deformator_path = os.path.join(root_dir, 'pretrained_models/deformators/ClosedForm/pggan_celebahq1024/pggan_celebahq1024.pkl')\n",
    "_, cf_deformator, _ = torch.load(deformator_path, map_location='cpu')\n",
    "cf_deformator = torch.FloatTensor(cf_deformator).cuda()\n",
    "\n",
    "deformator_path = os.path.join(root_dir, 'results/celeba_hq/closed_form_ours/models/18000_model.pkl')\n",
    "if algo == 'ortho':\n",
    "    dse_deformator = torch.load(deformator_path)['deformator']['ortho_mat']\n",
    "    dse_deformator = dse_deformator.T\n",
    "elif algo == 'linear':\n",
    "    deformator = torch.load(os.path.join(deformator_path))['deformator']\n",
    "    dse_deformator = deformator.T\n",
    "        \n",
    "generator = load_generator(None, model_name='pggan_celebahq1024')\n",
    "\n",
    "# if load_codes:\n",
    "#     codes = np.load(os.path.join(root_dir, 'pretrained_models/latent_codes/pggan_celebahq1024_latents.npy'))\n",
    "#     codes = torch.from_numpy(codes).type(torch.FloatTensor).cuda()\n",
    "#     codes = torch.load(os.path.join(root_dir, 'results/celeba_hq/closed_form_ours/quantitative_analysis/z_analysis.pkl'))\n",
    "# else:\n",
    "#     num_samples = 1000\n",
    "#     codes = torch.randn(num_samples, generator.z_space_dim).cuda()\n",
    "#     codes = generator.layer0.pixel_norm(codes)\n",
    "#     codes = codes.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instances for Pose and Gender\n",
    "\n",
    "# codes = []\n",
    "\n",
    "# num_samples=1000\n",
    "# z = torch.randn(num_samples, generator.z_space_dim).cuda()\n",
    "# z = generator.layer0.pixel_norm(z)\n",
    "# z = z.detach()\n",
    "# codes.append(z[996])\n",
    "# codes.append(z[953])\n",
    "\n",
    "# z = torch.load(os.path.join(root_dir, 'results/celeba_hq/closed_form_ours/quantitative_analysis/z_analysis.pkl'))\n",
    "# codes.append(z[45])\n",
    "# codes.append(z[119])\n",
    "# codes.append(z[207])\n",
    "# codes.append(z[250])\n",
    "# codes.append(z[308])\n",
    "\n",
    "# z = np.load(os.path.join(root_dir, 'pretrained_models/latent_codes/pggan_celebahq1024_latents.npy'))\n",
    "# z = torch.from_numpy(z).type(torch.FloatTensor).cuda()\n",
    "# codes.append(z[4])\n",
    "\n",
    "\n",
    "# codes = torch.stack(codes)\n",
    "\n",
    "# Instances for Smile\n",
    "\n",
    "# codes = []\n",
    "\n",
    "# num_samples=1000\n",
    "# z = torch.randn(num_samples, generator.z_space_dim).cuda()\n",
    "# z = generator.layer0.pixel_norm(z)\n",
    "# z = z.detach()\n",
    "# indices = [42, 43, 45, 56, 64, 83, 110, 115, 163]\n",
    "# codes = z[indices]\n",
    "\n",
    "#Instances for Age\n",
    "\n",
    "# codes = []\n",
    "\n",
    "# num_samples=1000\n",
    "# z = torch.randn(num_samples, generator.z_space_dim).cuda()\n",
    "# z = generator.layer0.pixel_norm(z)\n",
    "# z = z.detach()\n",
    "# indices = [0,6,50,224]\n",
    "# codes = z[indices]\n",
    "\n",
    "\n",
    "\n",
    "# Instances for glasses\n",
    "\n",
    "# codes = []\n",
    "\n",
    "# num_samples=1000\n",
    "# z = torch.randn(num_samples, generator.z_space_dim).cuda()\n",
    "# z = generator.layer0.pixel_norm(z)\n",
    "# z = z.detach()\n",
    "# indices = [97, 107, 111, 161, 243, 275, 308, 310, 365]\n",
    "# codes = z[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def postprocess_images(images):\n",
    "        \"\"\"Post-processes images from `torch.Tensor` to `numpy.ndarray`.\"\"\"\n",
    "        images = images.detach().cpu().numpy()\n",
    "        images = (images + 1) * 255 / 2\n",
    "        images = np.clip(images + 0.5, 0, 255).astype(np.uint8)\n",
    "        images = images.transpose(0, 2, 3, 1)\n",
    "        return images\n",
    "\n",
    "\n",
    "def save_images(codes, shifts_r, shifts_count, cf_dir, dse_dir, generator, cf_deformator, dse_deformator, min_index):\n",
    "        plt.figure(figsize=(30,30))\n",
    "        temp_path =  os.path.join(result_path, 'temp_glasses')\n",
    "        os.makedirs(temp_path, exist_ok=True)\n",
    "        torch.save(codes, os.path.join(temp_path, 'codes.pkl'))\n",
    "        for idx, z in enumerate(codes):\n",
    "#             print('Figure : ' + str(idx))\n",
    "            z_shift_cf = []\n",
    "            z_shift_dse = []\n",
    "            for i, shift in enumerate(np.linspace(-shifts_r,shifts_r,shifts_count)):\n",
    "                z_shift_cf.append(z + cf_deformator[cf_dir: cf_dir + 1] * shift)\n",
    "                z_shift_dse.append(z + dse_deformator[dse_dir: dse_dir + 1] * shift)\n",
    "            z_shift_cf = torch.stack(z_shift_cf).squeeze(dim=1)\n",
    "            z_shift_dse = torch.stack(z_shift_dse).squeeze(dim=1)\n",
    "            with torch.no_grad():\n",
    "                cf_images= generator(z_shift_cf)\n",
    "            torch.save(cf_images, os.path.join(temp_path, 'cf.pkl'))\n",
    "            del cf_images\n",
    "            with torch.no_grad():\n",
    "                dse_images= generator(z_shift_dse)\n",
    "            torch.save(dse_images, os.path.join(temp_path, 'dse.pkl'))\n",
    "            del dse_images\n",
    "            cf_images = torch.load(os.path.join(temp_path, 'cf.pkl'))\n",
    "            dse_images = torch.load(os.path.join(temp_path, 'dse.pkl'))\n",
    "            all_images = torch.cat((cf_images, dse_images), dim=0)\n",
    "            grid = torchvision.utils.make_grid(all_images.clamp(min=-1, max=1),nrow=3, scale_each=True, normalize=True)\n",
    "#             display.display(plt.gcf())\n",
    "            plt.grid(b=None)\n",
    "#             plt.axis('off')\n",
    "            plt.imsave(os.path.join(temp_path, str(min_index) + '.png'), grid.permute(1, 2, 0).cpu().numpy())\n",
    "            min_index = min_index + 1\n",
    "            del all_images\n",
    "            del cf_images\n",
    "            del dse_images\n",
    "            del grid\n",
    "    \n",
    "z_min_index =0\n",
    "z_max_index = 1000\n",
    "cf_dir = 1\n",
    "dse_dir = 14\n",
    "shift_r = 5\n",
    "shift_count = 3\n",
    "# codes = codes[z_min_index : z_max_index]\n",
    "# result_path = '/media/adarsh/DATA/CelebA-Analysis'\n",
    "# codes  = os.path.join(result_path, 'temp_smiling')\n",
    "# codes = torch.load(os.path.join(codes, 'codes.pkl'))\n",
    "all_images = save_images(codes, shift_r, shift_count, cf_dir, dse_dir, generator, cf_deformator, dse_deformator, z_min_index)                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_manipulated_images(z, shift_r, shift_count, cf_dir, dse_dir, generator, cf_deformator, dse_deformator):\n",
    "    temp_path =  os.path.join(result_path, 'temp')\n",
    "    os.makedirs(temp_path, exist_ok=True)\n",
    "    z_shift_cf = []\n",
    "    z_shift_dse = []\n",
    "    for i, shift in enumerate(np.linspace(-shifts_r,shifts_r,shifts_count)):\n",
    "        z_shift_cf.append(z + cf_deformator[cf_dir: cf_dir + 1] * shift)\n",
    "        z_shift_dse.append(z + dse_deformator[dse_dir: dse_dir + 1] * shift)\n",
    "    z_shift_cf = torch.stack(z_shift_cf).squeeze(dim=1)\n",
    "    z_shift_dse = torch.stack(z_shift_dse).squeeze(dim=1)\n",
    "    with torch.no_grad():\n",
    "        cf_images= generator(z_shift_cf)\n",
    "    with torch.no_grad():\n",
    "        dse_images= generator(z_shift_dse)\n",
    "    return cf_images, dse_images\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir= '/home/adarsh/PycharmProjects/disentagled_latent_dirs'\n",
    "result_path = os.path.join(root_dir,  'results/celeba_hq/closed_form_ours/qualitative_analysis')\n",
    "attr_list = ['Gender', 'Smiling', 'Glasses']\n",
    "z = []\n",
    "for each_attr in attr_list:\n",
    "    z.append(torch.load(os.path.join(result_path, each_attr, 'codes.pkl')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shifts_r = 6\n",
    "shifts_count = 3\n",
    "cf_dir = 1\n",
    "dse_dir = 1\n",
    "desired_idx  = 1\n",
    "\n",
    "cf_gender, dse_gender = get_manipulated_images(z[0][desired_idx], shifts_r, shifts_count, cf_dir, dse_dir, generator, cf_deformator, dse_deformator)\n",
    "\n",
    "\n",
    "shifts_r = 3\n",
    "shifts_count = 3\n",
    "cf_dir = 4\n",
    "dse_dir = 5\n",
    "desired_idx  = 7\n",
    "\n",
    "cf_smiling, dse_smiling = get_manipulated_images(z[1][desired_idx], shifts_r, shifts_count, cf_dir, dse_dir, generator, cf_deformator, dse_deformator)\n",
    "\n",
    "\n",
    "shifts_r = 5\n",
    "shifts_count = 3\n",
    "cf_dir = 1\n",
    "dse_dir = 14\n",
    "desired_idx  = 3\n",
    "\n",
    "cf_glass, dse_glass = get_manipulated_images(z[2][desired_idx], shifts_r, shifts_count, cf_dir, dse_dir, generator, cf_deformator, dse_deformator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = torch.stack((cf_gender, cf_smiling, cf_glass),dim=0)\n",
    "dse = torch.stack((dse_gender, dse_smiling, dse_glass),dim=0)\n",
    "all_images = [cf, dse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = ['SeFa', 'SeFa + SRE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "SMALL_SIZE = 8\n",
    "plt.rc('axes', titlesize=22, labelsize=20)\n",
    "plt.rcParams[\"figure.facecolor\"] = 'w'\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "gs = gridspec.GridSpec(2, 3, wspace=0.1, hspace=0.01)\n",
    "ax = np.zeros(6, dtype=object)\n",
    "count = 0\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        ax[count] = fig.add_subplot(gs[i, j])\n",
    "        grid = torchvision.utils.make_grid(all_images[i][j].clamp(min=-1, max=1),nrow=3, scale_each=True, normalize=True)\n",
    "        ax[count].imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "        ax[count].grid(False)\n",
    "        ax[count].set_xticks([])\n",
    "        ax[count].set_yticks([])\n",
    "        count = count + 1\n",
    "        ax[j].title.set_text(attr_list[j])\n",
    "ax[0].set_ylabel(algo[0], rotation=90)\n",
    "ax[3].set_ylabel(algo[1], rotation=90)\n",
    "\n",
    "\n",
    "gs.tight_layout(fig)\n",
    "plt.savefig('test.pdf', bbox_inches = 'tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
