{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize\n",
    "from utils import *\n",
    "from models.gan_load import make_style_gan2\n",
    "from models.gan_load import make_proggan\n",
    "from models.latent_deformator import LatentDeformator\n",
    "\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18\n",
    "import torch.nn as nn\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_type = 'prog-gan-sefa'\n",
    "num_directions = 512\n",
    "shifts_r= 10\n",
    "shifts_count=5\n",
    "scale = Resize(64)\n",
    "set_seed(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gan_type == 'StyleGAN2':\n",
    "    config_gan = {\"latent\": 512, \"n_mlp\": 3,\n",
    "                  \"channel_multiplier\": 8}\n",
    "    G = Generator(size=64, style_dim=config_gan[\"latent\"],n_mlp=config_gan[\"n_mlp\"],\n",
    "        small=True, channel_multiplier=config_gan[\"channel_multiplier\"])\n",
    "    G.load_state_dict(torch.load(opt.pretrained_gen_path))\n",
    "    G.eval().to(device)\n",
    "    for p in G.parameters():\n",
    "        p.requires_grad_(False)\n",
    "elif gan_type == 'StyleGAN2-Natural':\n",
    "    G = make_style_gan2(1024,'models/pretrained/generators/StyleGAN2/stylegan2-ffhq-config-f.pt' , True)\n",
    "elif gan_type == 'prog-gan':\n",
    "    G = make_proggan('models/pretrained/generators/ProgGAN/100_celeb_hq_network-snapshot-010403.pth').cuda()\n",
    "elif gan_type == 'prog-gan-sefa':\n",
    "    from models.proggan_sefa import PGGANGenerator\n",
    "    G = PGGANGenerator(resolution=1024)\n",
    "    checkpoint = torch.load('models/pretrained/ProgGAN/pggan_celebahq1024.pth', map_location='cpu')\n",
    "    if 'generator_smooth' in checkpoint:\n",
    "        G.load_state_dict(checkpoint['generator_smooth'])\n",
    "    else:\n",
    "        G.load_state_dict(checkpoint['generator'])\n",
    "    G = G.cuda()\n",
    "    G.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualisation_data_path = '/media/adarsh/DATA/CelebA-Analysis/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDeformator()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model = torch.load(visualisation_data_path + 'models/100007_model.pkl', map_location='cpu')\n",
    "deformator_ours = LatentDeformator(shift_dim=G.z_space_dim, input_dim=num_directions,\n",
    "                              out_dim=G.z_space_dim, type='ortho',random_init=True)\n",
    "deformator_ours.load_state_dict(pretrained_model['deformator'])\n",
    "deformator_ours.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDeformator(\n",
       "  (linear): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model = torch.load(visualisation_data_path + 'models/cf_model.pkl', map_location='cpu')\n",
    "deformator_cf = LatentDeformator(shift_dim=G.z_space_dim, input_dim=num_directions,\n",
    "                              out_dim=G.z_space_dim, type='linear',random_init=True, bias=False)\n",
    "\n",
    "deformator_cf.load_state_dict(pretrained_model['deformator'])\n",
    "deformator_cf.cuda()\n",
    "\n",
    "deformator_cf.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(z, shifts_r, shifts_count, direction, G, deformator, path):\n",
    "    for i,shift in enumerate(np.arange(-shifts_r, shifts_r, shifts_r / shifts_count)):\n",
    "        latent_shift = deformator(one_hot(deformator.input_dim, shift,direction).cuda())\n",
    "        shifted_image = G(z+latent_shift.cuda())\n",
    "        images_row = postprocess(shifted_image)\n",
    "        images_row = images_row.reshape(1024,1024,3)\n",
    "        cv2.imwrite(path +'image_' + str(i) + '.jpg', images_row[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(images):\n",
    "        \"\"\"Post-processes images from `torch.Tensor` to `numpy.ndarray`.\"\"\"\n",
    "        images = images.detach().cpu().numpy()\n",
    "        images = (images + 1) * 255 / 2\n",
    "        images = np.clip(images + 0.5, 0, 255).astype(np.uint8)\n",
    "        images = images.transpose(0, 2, 3, 1)\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = visualisation_data_path + '/their images'\n",
    "# codes = np.load('pggan_celebahq1024_latents.npy')\n",
    "# codes = torch.from_numpy(codes).type(torch.FloatTensor).cuda()\n",
    "# codes = codes.view(-1, 512)\n",
    "# images = torch.FloatTensor().cuda()\n",
    "# for i, each in enumerate(codes):\n",
    "#     image = G(each.view(-1, 512))\n",
    "#     images_row = postprocess(image)\n",
    "#     images_row = images_row.reshape(1024,1024,3)\n",
    "#     cv2.imwrite(path +'image_' + str(i) + '.jpg', images_row[:, :, ::-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = np.load('pggan_celebahq1024_latents.npy')\n",
    "codes = torch.from_numpy(codes).type(torch.FloatTensor).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54726729\n"
     ]
    }
   ],
   "source": [
    "seed = random.randint(0, 100000000)\n",
    "print(seed)\n",
    "set_seed(seed)\n",
    "# z = torch.randn(1, G.z_space_dim).cuda()\n",
    "z = codes[2].view(-1,512)\n",
    "direction = 70\n",
    "cf_image_path = 'images_cf/'\n",
    "ours_image_path = 'images_ours/'\n",
    "save_images(z, shifts_r, shifts_count, direction, G, deformator_cf, cf_image_path)\n",
    "save_images(z, shifts_r, shifts_count, direction, G, deformator_ours, ours_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Attribute Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = 'pose' ## eyeglasses, male, pose, smiling, young"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet():\n",
    "    net = resnet18()\n",
    "    modified_net = nn.Sequential(*list(net.children())[:-1])  # fetch all of the layers before the last fc.\n",
    "    return modified_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyModel(nn.Module):\n",
    "    def __init__(self, n_class=2):\n",
    "        super(ClassifyModel, self).__init__()\n",
    "        self.backbone = get_resnet()\n",
    "        self.extra_layer = nn.Linear(512, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.backbone(x)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.extra_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier(pretrain_path, device):\n",
    "    classifier = ClassifyModel().to(device)\n",
    "    classifier.load_state_dict(torch.load(pretrain_path))    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pose_predictor = get_classifier(os.path.join(visualisation_data_path, \"pretrain/classifier\", 'pose', \"weight.pkl\"), 'cpu')\n",
    "pose_predictor.eval()\n",
    "gender_predictor = get_classifier(os.path.join(visualisation_data_path, \"pretrain/classifier\", 'male', \"weight.pkl\"), 'cpu')\n",
    "gender_predictor.eval()\n",
    "age_predictor = get_classifier(os.path.join(visualisation_data_path, \"pretrain/classifier\", 'young', \"weight.pkl\"), 'cpu')\n",
    "age_predictor.eval()\n",
    "glasses_predictor = get_classifier(os.path.join(visualisation_data_path, \"pretrain/classifier\", 'eyeglasses', \"weight.pkl\"), 'cpu')\n",
    "glasses_predictor.eval()\n",
    "smile_predictor = get_classifier(os.path.join(visualisation_data_path, \"pretrain/classifier\", 'smiling', \"weight.pkl\"), 'cpu')\n",
    "smile_predictor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = 10 ## Set to 1 for a fair comparison in Closed Form Issues\n",
    "direction = 499\n",
    "total_images = 10\n",
    "images_per_batch = 1\n",
    "save_image_dir = os.path.join(visualisation_data_path, \"images/images_new_direction_\"+str(direction))\n",
    "if not os.path.exists(save_image_dir):\n",
    "    os.makedirs(save_image_dir)\n",
    "for i in range(int(total_images/ images_per_batch)):\n",
    "    print('Image_' + str(i))\n",
    "#     z = torch.randn(images_per_batch, G.z_space_dim).cuda()\n",
    "    z = codes[i].view(-1, 512)\n",
    "    image = G(z)\n",
    "    image = F.avg_pool2d(image, 4, 4)\n",
    "    predictions_img = torch.softmax(gender_predictor(image.cpu()),dim=1)\n",
    "    print('Image scores : ')\n",
    "    print(predictions_img)\n",
    "    latent_shift = deformator_cf(one_hot(deformator_cf.input_dim, shift,direction).cuda())\n",
    "    image_shifted =  G(z+latent_shift.cuda())\n",
    "    image_shifted = F.avg_pool2d(image_shifted, 4, 4)\n",
    "    predictions_img_shift = torch.softmax(gender_predictor(image_shifted.cpu()),dim=1)\n",
    "    print('Image shifted scores : ')\n",
    "    print(predictions_img_shift)\n",
    "    torch.save((image, image_shifted), save_image_dir + '/image_'+str(i)+'.pkl')\n",
    "    del image\n",
    "    del image_shifted\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions_img_shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "num_images = 10\n",
    "all_images = torch.FloatTensor().cuda()\n",
    "for i in range(num_images):\n",
    "    images = torch.load(visualisation_data_path + '/images/images_new_direction_'+str(direction) + '/image_'+str(i)+'.pkl')\n",
    "    image = torch.cat((images[0], images[1])).detach()\n",
    "    all_images = torch.cat((all_images, image))\n",
    "    del image, images\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "plt.figure(figsize= (60,60))\n",
    "grid = torchvision.utils.make_grid(all_images.clamp(min=-1, max=1),nrow=2, scale_each=True, normalize=True)\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "images = []\n",
    "for img_path in glob.glob('images/*.png'):\n",
    "    images.append(mpimg.imread(img_path))\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "columns = 10\n",
    "for i, image in enumerate(images):\n",
    "    plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
    "    plt.imshow(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
